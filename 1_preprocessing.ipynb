{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4975aa8a",
      "metadata": {
        "id": "4975aa8a"
      },
      "outputs": [],
      "source": [
        "# --- Install and Fixes ---\n",
        "!pip install numpy==1.26.4 --quiet\n",
        "!pip install -q --upgrade transformers datasets accelerate evaluate rouge_score\n",
        "import os\n",
        "os.kill(os.getpid(), 9)  # Restart runtime to apply numpy fix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "968a8aa0",
      "metadata": {
        "id": "968a8aa0"
      },
      "outputs": [],
      "source": [
        "# --- Imports ---\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from datasets import load_dataset, Dataset\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "from torch.optim import AdamW\n",
        "import pandas as pd\n",
        "import json\n",
        "from accelerate import Accelerator\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2dba85e6",
      "metadata": {
        "id": "2dba85e6"
      },
      "outputs": [],
      "source": [
        "# --- Load Raw Data ---\n",
        "with open(\"train.src.cleaned\", \"r\") as f:\n",
        "    docs = [line.strip() for _, line in zip(range(500), f)]\n",
        "\n",
        "with open(\"train.tgt\", \"r\") as f:\n",
        "    summaries = [line.strip() for _, line in zip(range(500), f)]\n",
        "\n",
        "print(\"Docs:\", len(docs))\n",
        "print(\"Summaries:\", len(summaries))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16f3f987",
      "metadata": {
        "id": "16f3f987"
      },
      "outputs": [],
      "source": [
        "# --- Clean + Create DataFrame ---\n",
        "min_len = min(len(docs), len(summaries))\n",
        "docs = docs[:min_len]\n",
        "summaries = summaries[:min_len]\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"document\": [doc.strip() for doc in docs],\n",
        "    \"summary\": [summary.strip() for summary in summaries]\n",
        "})\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58c82ec9",
      "metadata": {
        "id": "58c82ec9"
      },
      "outputs": [],
      "source": [
        "# --- Convert to Dataset ---\n",
        "dataset = Dataset.from_pandas(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3d6d4d77",
      "metadata": {
        "id": "3d6d4d77"
      },
      "outputs": [],
      "source": [
        "# --- Tokenizer ---\n",
        "from transformers import AutoTokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"t5-small\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5cf2d5e",
      "metadata": {
        "id": "c5cf2d5e"
      },
      "outputs": [],
      "source": [
        "# --- Preprocessing Function ---\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"summarize: \" + doc for doc in examples[\"document\"]]\n",
        "    model_inputs = tokenizer(inputs, max_length=512, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(examples[\"summary\"], max_length=128, truncation=True, padding=\"max_length\")\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f2cffb9",
      "metadata": {
        "id": "5f2cffb9"
      },
      "outputs": [],
      "source": [
        "# --- Apply Preprocessing ---\n",
        "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2cb97faf",
      "metadata": {
        "id": "2cb97faf"
      },
      "outputs": [],
      "source": [
        "# --- Save Sample Tokenized Subset ---\n",
        "small_dataset = tokenized_dataset.select(range(500))\n",
        "small_dataset.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "small_dataset.save_to_disk(\"tokenized_sample_dataset\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}